{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.31043779836\n",
      "=== epoch:1, train acc:0.0866666666667, test acc:0.0785 ===\n",
      "train loss:2.30527155529\n",
      "train loss:2.28662984026\n",
      "train loss:2.29986592594\n",
      "=== epoch:2, train acc:0.0866666666667, test acc:0.0771 ===\n",
      "train loss:2.31155237983\n",
      "train loss:2.28028630951\n",
      "train loss:2.30627339803\n",
      "=== epoch:3, train acc:0.0866666666667, test acc:0.0758 ===\n",
      "train loss:2.30083784513\n",
      "train loss:2.29390134732\n",
      "train loss:2.31018484301\n",
      "=== epoch:4, train acc:0.0866666666667, test acc:0.075 ===\n",
      "train loss:2.29565521031\n",
      "train loss:2.29691514689\n",
      "train loss:2.28024135432\n",
      "=== epoch:5, train acc:0.0866666666667, test acc:0.0721 ===\n",
      "train loss:2.29377576185\n",
      "train loss:2.30501068177\n",
      "train loss:2.28917451136\n",
      "=== epoch:6, train acc:0.0833333333333, test acc:0.0702 ===\n",
      "train loss:2.29842935401\n",
      "train loss:2.28733033574\n",
      "train loss:2.29400199547\n",
      "=== epoch:7, train acc:0.0833333333333, test acc:0.0713 ===\n",
      "train loss:2.30431548643\n",
      "train loss:2.29447420377\n",
      "train loss:2.2981357064\n",
      "=== epoch:8, train acc:0.0833333333333, test acc:0.074 ===\n",
      "train loss:2.27127161549\n",
      "train loss:2.28324350556\n",
      "train loss:2.28762559539\n",
      "=== epoch:9, train acc:0.09, test acc:0.0775 ===\n",
      "train loss:2.29853630941\n",
      "train loss:2.27988345928\n",
      "train loss:2.27078119543\n",
      "=== epoch:10, train acc:0.0933333333333, test acc:0.0785 ===\n",
      "train loss:2.27248027103\n",
      "train loss:2.27977325298\n",
      "train loss:2.28718267697\n",
      "=== epoch:11, train acc:0.0933333333333, test acc:0.0791 ===\n",
      "train loss:2.28075970054\n",
      "train loss:2.30492657175\n",
      "train loss:2.28901608199\n",
      "=== epoch:12, train acc:0.09, test acc:0.0786 ===\n",
      "train loss:2.29189873801\n",
      "train loss:2.28786453439\n",
      "train loss:2.29262312089\n",
      "=== epoch:13, train acc:0.0933333333333, test acc:0.0803 ===\n",
      "train loss:2.2851103095\n",
      "train loss:2.27721682152\n",
      "train loss:2.27589903328\n",
      "=== epoch:14, train acc:0.09, test acc:0.0813 ===\n",
      "train loss:2.26567062963\n",
      "train loss:2.29307254284\n",
      "train loss:2.30853369773\n",
      "=== epoch:15, train acc:0.0866666666667, test acc:0.0821 ===\n",
      "train loss:2.2859280773\n",
      "train loss:2.28807668611\n",
      "train loss:2.28358346689\n",
      "=== epoch:16, train acc:0.106666666667, test acc:0.0866 ===\n",
      "train loss:2.28274531668\n",
      "train loss:2.28731207497\n",
      "train loss:2.29682645871\n",
      "=== epoch:17, train acc:0.11, test acc:0.0888 ===\n",
      "train loss:2.28307584979\n",
      "train loss:2.28273321669\n",
      "train loss:2.27680230836\n",
      "=== epoch:18, train acc:0.116666666667, test acc:0.0913 ===\n",
      "train loss:2.29460017351\n",
      "train loss:2.28330339649\n",
      "train loss:2.28780280187\n",
      "=== epoch:19, train acc:0.11, test acc:0.0934 ===\n",
      "train loss:2.27521093424\n",
      "train loss:2.28594851549\n",
      "train loss:2.2672691284\n",
      "=== epoch:20, train acc:0.11, test acc:0.0943 ===\n",
      "train loss:2.284768274\n",
      "train loss:2.27397447157\n",
      "train loss:2.28078231405\n",
      "=== epoch:21, train acc:0.113333333333, test acc:0.0973 ===\n",
      "train loss:2.27162760075\n",
      "train loss:2.27152380976\n",
      "train loss:2.27135822736\n",
      "=== epoch:22, train acc:0.12, test acc:0.1027 ===\n",
      "train loss:2.25848828804\n",
      "train loss:2.27563155214\n",
      "train loss:2.27731132144\n",
      "=== epoch:23, train acc:0.126666666667, test acc:0.1063 ===\n",
      "train loss:2.27044751687\n",
      "train loss:2.27373518951\n",
      "train loss:2.26968883425\n",
      "=== epoch:24, train acc:0.133333333333, test acc:0.1098 ===\n",
      "train loss:2.28084262149\n",
      "train loss:2.27012436295\n",
      "train loss:2.27874452055\n",
      "=== epoch:25, train acc:0.14, test acc:0.114 ===\n",
      "train loss:2.28470596676\n",
      "train loss:2.27917735161\n",
      "train loss:2.27757813303\n",
      "=== epoch:26, train acc:0.13, test acc:0.1139 ===\n",
      "train loss:2.27338423849\n",
      "train loss:2.27535157785\n",
      "train loss:2.26339938383\n",
      "=== epoch:27, train acc:0.133333333333, test acc:0.1192 ===\n",
      "train loss:2.26469993448\n",
      "train loss:2.28555038843\n",
      "train loss:2.27598343656\n",
      "=== epoch:28, train acc:0.14, test acc:0.1239 ===\n",
      "train loss:2.27351725254\n",
      "train loss:2.26686218676\n",
      "train loss:2.27001995518\n",
      "=== epoch:29, train acc:0.143333333333, test acc:0.1283 ===\n",
      "train loss:2.27084938853\n",
      "train loss:2.27082746816\n",
      "train loss:2.27349099975\n",
      "=== epoch:30, train acc:0.17, test acc:0.1376 ===\n",
      "train loss:2.27433942281\n",
      "train loss:2.26631936529\n",
      "train loss:2.26907459707\n",
      "=== epoch:31, train acc:0.176666666667, test acc:0.1368 ===\n",
      "train loss:2.26451528968\n",
      "train loss:2.26493112795\n",
      "train loss:2.27683548498\n",
      "=== epoch:32, train acc:0.183333333333, test acc:0.142 ===\n",
      "train loss:2.27487744724\n",
      "train loss:2.26685584953\n",
      "train loss:2.26234447671\n",
      "=== epoch:33, train acc:0.183333333333, test acc:0.1527 ===\n",
      "train loss:2.26660017576\n",
      "train loss:2.25952868262\n",
      "train loss:2.26747859506\n",
      "=== epoch:34, train acc:0.193333333333, test acc:0.1606 ===\n",
      "train loss:2.26686731647\n",
      "train loss:2.27203056938\n",
      "train loss:2.26154150926\n",
      "=== epoch:35, train acc:0.193333333333, test acc:0.166 ===\n",
      "train loss:2.26981929885\n",
      "train loss:2.26522261896\n",
      "train loss:2.26952454691\n",
      "=== epoch:36, train acc:0.196666666667, test acc:0.1672 ===\n",
      "train loss:2.25526331112\n",
      "train loss:2.25630936197\n",
      "train loss:2.27078851435\n",
      "=== epoch:37, train acc:0.2, test acc:0.1687 ===\n",
      "train loss:2.25939812419\n",
      "train loss:2.25829651512\n",
      "train loss:2.25198569291\n",
      "=== epoch:38, train acc:0.2, test acc:0.1733 ===\n",
      "train loss:2.26867242169\n",
      "train loss:2.25979638648\n",
      "train loss:2.26958302152\n",
      "=== epoch:39, train acc:0.196666666667, test acc:0.1771 ===\n",
      "train loss:2.27694524218\n",
      "train loss:2.24947351145\n",
      "train loss:2.26838572358\n",
      "=== epoch:40, train acc:0.21, test acc:0.1801 ===\n",
      "train loss:2.25680553045\n",
      "train loss:2.27124940482\n",
      "train loss:2.26186354213\n",
      "=== epoch:41, train acc:0.21, test acc:0.1744 ===\n",
      "train loss:2.2718421635\n",
      "train loss:2.25343172977\n",
      "train loss:2.2495229652\n",
      "=== epoch:42, train acc:0.206666666667, test acc:0.1782 ===\n",
      "train loss:2.26508871181\n",
      "train loss:2.26383003587\n",
      "train loss:2.26091569983\n",
      "=== epoch:43, train acc:0.223333333333, test acc:0.1808 ===\n",
      "train loss:2.25282775917\n",
      "train loss:2.24896610749\n",
      "train loss:2.26873280126\n",
      "=== epoch:44, train acc:0.22, test acc:0.1833 ===\n",
      "train loss:2.25568802845\n",
      "train loss:2.26174725544\n",
      "train loss:2.25239952977\n",
      "=== epoch:45, train acc:0.223333333333, test acc:0.1823 ===\n",
      "train loss:2.25568313874\n",
      "train loss:2.24400969275\n",
      "train loss:2.2474658327\n",
      "=== epoch:46, train acc:0.216666666667, test acc:0.1758 ===\n",
      "train loss:2.23783890297\n",
      "train loss:2.26076166126\n",
      "train loss:2.2673464192\n",
      "=== epoch:47, train acc:0.203333333333, test acc:0.1721 ===\n",
      "train loss:2.25414523569\n",
      "train loss:2.2565331463\n",
      "train loss:2.25293668828\n",
      "=== epoch:48, train acc:0.203333333333, test acc:0.1724 ===\n",
      "train loss:2.26717883872\n",
      "train loss:2.26040930772\n",
      "train loss:2.25074944742\n",
      "=== epoch:49, train acc:0.21, test acc:0.1731 ===\n",
      "train loss:2.24151234843\n",
      "train loss:2.25133935582\n",
      "train loss:2.24639348626\n",
      "=== epoch:50, train acc:0.216666666667, test acc:0.1737 ===\n",
      "train loss:2.25735423169\n",
      "train loss:2.25142924149\n",
      "train loss:2.24281145505\n",
      "=== epoch:51, train acc:0.216666666667, test acc:0.1751 ===\n",
      "train loss:2.23979338981\n",
      "train loss:2.24686892501\n",
      "train loss:2.23289635158\n",
      "=== epoch:52, train acc:0.216666666667, test acc:0.1705 ===\n",
      "train loss:2.23485340722\n",
      "train loss:2.2555492506\n",
      "train loss:2.24884158506\n",
      "=== epoch:53, train acc:0.206666666667, test acc:0.1695 ===\n",
      "train loss:2.25156504647\n",
      "train loss:2.24994758776\n",
      "train loss:2.25326975946\n",
      "=== epoch:54, train acc:0.223333333333, test acc:0.1777 ===\n",
      "train loss:2.25435055263\n",
      "train loss:2.25951045737\n",
      "train loss:2.252169948\n",
      "=== epoch:55, train acc:0.23, test acc:0.1776 ===\n",
      "train loss:2.24676774635\n",
      "train loss:2.25767968313\n",
      "train loss:2.25416467428\n",
      "=== epoch:56, train acc:0.23, test acc:0.1818 ===\n",
      "train loss:2.25378395179\n",
      "train loss:2.23965659301\n",
      "train loss:2.25200219322\n",
      "=== epoch:57, train acc:0.233333333333, test acc:0.1882 ===\n",
      "train loss:2.24985460467\n",
      "train loss:2.23747376992\n",
      "train loss:2.24465202807\n",
      "=== epoch:58, train acc:0.24, test acc:0.1878 ===\n",
      "train loss:2.24352446518\n",
      "train loss:2.23611628009\n",
      "train loss:2.25804613091\n",
      "=== epoch:59, train acc:0.243333333333, test acc:0.1922 ===\n",
      "train loss:2.23978706503\n",
      "train loss:2.23944482511\n",
      "train loss:2.23759018042\n",
      "=== epoch:60, train acc:0.246666666667, test acc:0.193 ===\n",
      "train loss:2.26499336076\n",
      "train loss:2.24678782062\n",
      "train loss:2.2413010574\n",
      "=== epoch:61, train acc:0.243333333333, test acc:0.1985 ===\n",
      "train loss:2.23963503471\n",
      "train loss:2.23744590963\n",
      "train loss:2.23914422899\n",
      "=== epoch:62, train acc:0.246666666667, test acc:0.1984 ===\n",
      "train loss:2.23878149799\n",
      "train loss:2.22717286814\n",
      "train loss:2.21225449472\n",
      "=== epoch:63, train acc:0.243333333333, test acc:0.1937 ===\n",
      "train loss:2.23216634329\n",
      "train loss:2.23661691872\n",
      "train loss:2.23128100574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:64, train acc:0.26, test acc:0.2036 ===\n",
      "train loss:2.22751973954\n",
      "train loss:2.23251303907\n",
      "train loss:2.21401757169\n",
      "=== epoch:65, train acc:0.256666666667, test acc:0.1973 ===\n",
      "train loss:2.21717636233\n",
      "train loss:2.24743752347\n",
      "train loss:2.21447673886\n",
      "=== epoch:66, train acc:0.263333333333, test acc:0.2014 ===\n",
      "train loss:2.20656925353\n",
      "train loss:2.23074242385\n",
      "train loss:2.24470794289\n",
      "=== epoch:67, train acc:0.26, test acc:0.2031 ===\n",
      "train loss:2.21771451234\n",
      "train loss:2.23231741749\n",
      "train loss:2.23462953099\n",
      "=== epoch:68, train acc:0.246666666667, test acc:0.1987 ===\n",
      "train loss:2.24012907001\n",
      "train loss:2.21186335007\n",
      "train loss:2.24430581082\n",
      "=== epoch:69, train acc:0.266666666667, test acc:0.2062 ===\n",
      "train loss:2.2362263278\n",
      "train loss:2.23740859796\n",
      "train loss:2.22122063529\n",
      "=== epoch:70, train acc:0.266666666667, test acc:0.2102 ===\n",
      "train loss:2.2432921578\n",
      "train loss:2.20795590206\n",
      "train loss:2.206354582\n",
      "=== epoch:71, train acc:0.246666666667, test acc:0.2031 ===\n",
      "train loss:2.22005816132\n",
      "train loss:2.25714436712\n",
      "train loss:2.22678170079\n",
      "=== epoch:72, train acc:0.27, test acc:0.2078 ===\n",
      "train loss:2.2234351203\n",
      "train loss:2.22474272947\n",
      "train loss:2.24669264955\n",
      "=== epoch:73, train acc:0.283333333333, test acc:0.213 ===\n",
      "train loss:2.21673752177\n",
      "train loss:2.24063426828\n",
      "train loss:2.22676483825\n",
      "=== epoch:74, train acc:0.283333333333, test acc:0.2133 ===\n",
      "train loss:2.22641198713\n",
      "train loss:2.22152662755\n",
      "train loss:2.24234783023\n",
      "=== epoch:75, train acc:0.29, test acc:0.2149 ===\n",
      "train loss:2.21843247777\n",
      "train loss:2.18756540939\n",
      "train loss:2.22336597397\n",
      "=== epoch:76, train acc:0.296666666667, test acc:0.2188 ===\n",
      "train loss:2.23135375327\n",
      "train loss:2.22376721417\n",
      "train loss:2.22500597631\n",
      "=== epoch:77, train acc:0.296666666667, test acc:0.2201 ===\n",
      "train loss:2.24515048401\n",
      "train loss:2.2030028978\n",
      "train loss:2.22213463198\n",
      "=== epoch:78, train acc:0.303333333333, test acc:0.2228 ===\n",
      "train loss:2.22959918897\n",
      "train loss:2.19396268187\n",
      "train loss:2.21645875966\n",
      "=== epoch:79, train acc:0.3, test acc:0.2237 ===\n",
      "train loss:2.22235052497\n",
      "train loss:2.21014509392\n",
      "train loss:2.20174463466\n",
      "=== epoch:80, train acc:0.3, test acc:0.232 ===\n",
      "train loss:2.22413499963\n",
      "train loss:2.21060271329\n",
      "train loss:2.2164141894\n",
      "=== epoch:81, train acc:0.313333333333, test acc:0.2387 ===\n",
      "train loss:2.20449662774\n",
      "train loss:2.19451872462\n",
      "train loss:2.21864945511\n",
      "=== epoch:82, train acc:0.31, test acc:0.2326 ===\n",
      "train loss:2.21652464871\n",
      "train loss:2.21306172079\n",
      "train loss:2.20591506818\n",
      "=== epoch:83, train acc:0.32, test acc:0.2353 ===\n",
      "train loss:2.20926594093\n",
      "train loss:2.21168528397\n",
      "train loss:2.21480530861\n",
      "=== epoch:84, train acc:0.32, test acc:0.2396 ===\n",
      "train loss:2.21012253125\n",
      "train loss:2.21145091736\n",
      "train loss:2.2123115863\n",
      "=== epoch:85, train acc:0.32, test acc:0.2429 ===\n",
      "train loss:2.19650229978\n",
      "train loss:2.19684826046\n",
      "train loss:2.20357119723\n",
      "=== epoch:86, train acc:0.326666666667, test acc:0.2479 ===\n",
      "train loss:2.1853065369\n",
      "train loss:2.19101045237\n",
      "train loss:2.20308120251\n",
      "=== epoch:87, train acc:0.33, test acc:0.2488 ===\n",
      "train loss:2.19876219694\n",
      "train loss:2.21711245523\n",
      "train loss:2.18793577666\n",
      "=== epoch:88, train acc:0.33, test acc:0.2492 ===\n",
      "train loss:2.21412758699\n",
      "train loss:2.21043132922\n",
      "train loss:2.20158965642\n",
      "=== epoch:89, train acc:0.333333333333, test acc:0.2533 ===\n",
      "train loss:2.18079612589\n",
      "train loss:2.21607062593\n",
      "train loss:2.22111134718\n",
      "=== epoch:90, train acc:0.336666666667, test acc:0.2585 ===\n",
      "train loss:2.2156433725\n",
      "train loss:2.20515174732\n",
      "train loss:2.18546749216\n",
      "=== epoch:91, train acc:0.333333333333, test acc:0.2578 ===\n",
      "train loss:2.19406970565\n",
      "train loss:2.17927091781\n",
      "train loss:2.17372114814\n",
      "=== epoch:92, train acc:0.343333333333, test acc:0.2618 ===\n",
      "train loss:2.1992166606\n",
      "train loss:2.18706850428\n",
      "train loss:2.22101970101\n",
      "=== epoch:93, train acc:0.35, test acc:0.2735 ===\n",
      "train loss:2.20094131723\n",
      "train loss:2.19548878726\n",
      "train loss:2.15066121099\n",
      "=== epoch:94, train acc:0.35, test acc:0.2726 ===\n",
      "train loss:2.17348643887\n",
      "train loss:2.18842442074\n",
      "train loss:2.17165850221\n",
      "=== epoch:95, train acc:0.343333333333, test acc:0.2707 ===\n",
      "train loss:2.16223179334\n",
      "train loss:2.19257217586\n",
      "train loss:2.18229048492\n",
      "=== epoch:96, train acc:0.346666666667, test acc:0.2742 ===\n",
      "train loss:2.18223222183\n",
      "train loss:2.18183575067\n",
      "train loss:2.18142127872\n",
      "=== epoch:97, train acc:0.346666666667, test acc:0.2788 ===\n",
      "train loss:2.16629117718\n",
      "train loss:2.18515148719\n",
      "train loss:2.1663725059\n",
      "=== epoch:98, train acc:0.346666666667, test acc:0.2751 ===\n",
      "train loss:2.17368403414\n",
      "train loss:2.17581973766\n",
      "train loss:2.19255149353\n",
      "=== epoch:99, train acc:0.35, test acc:0.2793 ===\n",
      "train loss:2.18147824644\n",
      "train loss:2.18675008618\n",
      "train loss:2.15964324928\n",
      "=== epoch:100, train acc:0.356666666667, test acc:0.2818 ===\n",
      "train loss:2.18219971548\n",
      "train loss:2.17094799016\n",
      "train loss:2.1790121968\n",
      "=== epoch:101, train acc:0.363333333333, test acc:0.2898 ===\n",
      "train loss:2.18300093718\n",
      "train loss:2.16428132884\n",
      "train loss:2.17096847887\n",
      "=== epoch:102, train acc:0.363333333333, test acc:0.2882 ===\n",
      "train loss:2.16664441166\n",
      "train loss:2.13277055456\n",
      "train loss:2.16252755374\n",
      "=== epoch:103, train acc:0.363333333333, test acc:0.2871 ===\n",
      "train loss:2.18265743308\n",
      "train loss:2.19184092601\n",
      "train loss:2.16533167607\n",
      "=== epoch:104, train acc:0.373333333333, test acc:0.2952 ===\n",
      "train loss:2.15112921728\n",
      "train loss:2.1563970378\n",
      "train loss:2.17399705973\n",
      "=== epoch:105, train acc:0.37, test acc:0.2935 ===\n",
      "train loss:2.15312524615\n",
      "train loss:2.14901409912\n",
      "train loss:2.14820892342\n",
      "=== epoch:106, train acc:0.386666666667, test acc:0.3003 ===\n",
      "train loss:2.15864236738\n",
      "train loss:2.15984548872\n",
      "train loss:2.15482333853\n",
      "=== epoch:107, train acc:0.386666666667, test acc:0.2983 ===\n",
      "train loss:2.17131313483\n",
      "train loss:2.16564328009\n",
      "train loss:2.1514926264\n",
      "=== epoch:108, train acc:0.403333333333, test acc:0.3099 ===\n",
      "train loss:2.14461762448\n",
      "train loss:2.13861771952\n",
      "train loss:2.13227871233\n",
      "=== epoch:109, train acc:0.403333333333, test acc:0.3111 ===\n",
      "train loss:2.12204642803\n",
      "train loss:2.14438520289\n",
      "train loss:2.15959525131\n",
      "=== epoch:110, train acc:0.406666666667, test acc:0.3112 ===\n",
      "train loss:2.13132269236\n",
      "train loss:2.1711776589\n",
      "train loss:2.15816940237\n",
      "=== epoch:111, train acc:0.413333333333, test acc:0.3171 ===\n",
      "train loss:2.15164455704\n",
      "train loss:2.13961708331\n",
      "train loss:2.16133192227\n",
      "=== epoch:112, train acc:0.416666666667, test acc:0.3187 ===\n",
      "train loss:2.15824177174\n",
      "train loss:2.13891581378\n",
      "train loss:2.11388188436\n",
      "=== epoch:113, train acc:0.413333333333, test acc:0.3155 ===\n",
      "train loss:2.13005671835\n",
      "train loss:2.10248913553\n",
      "train loss:2.12285805767\n",
      "=== epoch:114, train acc:0.426666666667, test acc:0.3216 ===\n",
      "train loss:2.13641390194\n",
      "train loss:2.12201510008\n",
      "train loss:2.08247237188\n",
      "=== epoch:115, train acc:0.426666666667, test acc:0.3199 ===\n",
      "train loss:2.13359822236\n",
      "train loss:2.12484733793\n",
      "train loss:2.0942986517\n",
      "=== epoch:116, train acc:0.423333333333, test acc:0.3218 ===\n",
      "train loss:2.10712313802\n",
      "train loss:2.13718734403\n",
      "train loss:2.11278413639\n",
      "=== epoch:117, train acc:0.42, test acc:0.3238 ===\n",
      "train loss:2.13565594861\n",
      "train loss:2.16232528509\n",
      "train loss:2.14872160149\n",
      "=== epoch:118, train acc:0.42, test acc:0.3278 ===\n",
      "train loss:2.1221438567\n",
      "train loss:2.11394277152\n",
      "train loss:2.11465081368\n",
      "=== epoch:119, train acc:0.426666666667, test acc:0.3348 ===\n",
      "train loss:2.0896922437\n",
      "train loss:2.09128080424\n",
      "train loss:2.10894991506\n",
      "=== epoch:120, train acc:0.416666666667, test acc:0.3339 ===\n",
      "train loss:2.1275572492\n",
      "train loss:2.13440619923\n",
      "train loss:2.1192442934\n",
      "=== epoch:121, train acc:0.433333333333, test acc:0.3444 ===\n",
      "train loss:2.08482973031\n",
      "train loss:2.12123820711\n",
      "train loss:2.04955099522\n",
      "=== epoch:122, train acc:0.433333333333, test acc:0.3362 ===\n",
      "train loss:2.11741250412\n",
      "train loss:2.11093387965\n",
      "train loss:2.10763843688\n",
      "=== epoch:123, train acc:0.433333333333, test acc:0.3454 ===\n",
      "train loss:2.13822512624\n",
      "train loss:2.09831197022\n",
      "train loss:2.12304428027\n",
      "=== epoch:124, train acc:0.456666666667, test acc:0.3553 ===\n",
      "train loss:2.09160243309\n",
      "train loss:2.11435470365\n",
      "train loss:2.12283918254\n",
      "=== epoch:125, train acc:0.443333333333, test acc:0.3534 ===\n",
      "train loss:2.11115898058\n",
      "train loss:2.07948633797\n",
      "train loss:2.1018276471\n",
      "=== epoch:126, train acc:0.446666666667, test acc:0.356 ===\n",
      "train loss:2.06528450956\n",
      "train loss:2.12214127463\n",
      "train loss:2.10061547711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:127, train acc:0.453333333333, test acc:0.3587 ===\n",
      "train loss:2.10559805513\n",
      "train loss:2.10873919202\n",
      "train loss:2.11570599406\n",
      "=== epoch:128, train acc:0.463333333333, test acc:0.369 ===\n",
      "train loss:2.04896230024\n",
      "train loss:2.13913027627\n",
      "train loss:2.07217484286\n",
      "=== epoch:129, train acc:0.47, test acc:0.3683 ===\n",
      "train loss:2.10430011913\n",
      "train loss:2.10318646729\n",
      "train loss:2.10729857024\n",
      "=== epoch:130, train acc:0.473333333333, test acc:0.3751 ===\n",
      "train loss:2.08047611082\n",
      "train loss:2.09764374734\n",
      "train loss:2.07986643817\n",
      "=== epoch:131, train acc:0.473333333333, test acc:0.3745 ===\n",
      "train loss:2.06935886649\n",
      "train loss:2.11244603932\n",
      "train loss:2.12948217853\n",
      "=== epoch:132, train acc:0.476666666667, test acc:0.3802 ===\n",
      "train loss:2.08188098022\n",
      "train loss:2.07511778079\n",
      "train loss:2.04031034735\n",
      "=== epoch:133, train acc:0.47, test acc:0.3784 ===\n",
      "train loss:2.09725462516\n",
      "train loss:2.06932958403\n",
      "train loss:2.06142688712\n",
      "=== epoch:134, train acc:0.473333333333, test acc:0.3758 ===\n",
      "train loss:2.05415386239\n",
      "train loss:2.05618767233\n",
      "train loss:2.1082972497\n",
      "=== epoch:135, train acc:0.48, test acc:0.3768 ===\n",
      "train loss:2.07188179005\n",
      "train loss:2.0445759089\n",
      "train loss:2.08159819639\n",
      "=== epoch:136, train acc:0.48, test acc:0.3798 ===\n",
      "train loss:2.04365250834\n",
      "train loss:2.05491443366\n",
      "train loss:2.10646869527\n",
      "=== epoch:137, train acc:0.486666666667, test acc:0.3848 ===\n",
      "train loss:2.02446315796\n",
      "train loss:2.07571491132\n",
      "train loss:2.09837442278\n",
      "=== epoch:138, train acc:0.493333333333, test acc:0.387 ===\n",
      "train loss:1.98370677301\n",
      "train loss:2.09248510616\n",
      "train loss:2.02575043063\n",
      "=== epoch:139, train acc:0.49, test acc:0.3862 ===\n",
      "train loss:2.05554995357\n",
      "train loss:2.06364734571\n",
      "train loss:2.01038850357\n",
      "=== epoch:140, train acc:0.48, test acc:0.3878 ===\n",
      "train loss:2.05189041889\n",
      "train loss:2.04260927945\n",
      "train loss:2.05776789265\n",
      "=== epoch:141, train acc:0.48, test acc:0.3942 ===\n",
      "train loss:2.04701112442\n",
      "train loss:2.09310929421\n",
      "train loss:2.01300020026\n",
      "=== epoch:142, train acc:0.48, test acc:0.3894 ===\n",
      "train loss:2.03185891922\n",
      "train loss:1.98827269868\n",
      "train loss:2.04013026282\n",
      "=== epoch:143, train acc:0.49, test acc:0.3947 ===\n",
      "train loss:2.06291841142\n",
      "train loss:2.00637800867\n",
      "train loss:1.99836995262\n",
      "=== epoch:144, train acc:0.466666666667, test acc:0.3903 ===\n",
      "train loss:2.01378409806\n",
      "train loss:1.99546847716\n",
      "train loss:2.06475885105\n",
      "=== epoch:145, train acc:0.483333333333, test acc:0.3977 ===\n",
      "train loss:1.99639381248\n",
      "train loss:2.15021809137\n",
      "train loss:2.01494894739\n",
      "=== epoch:146, train acc:0.49, test acc:0.4015 ===\n",
      "train loss:1.99818435523\n",
      "train loss:2.03912021016\n",
      "train loss:2.04360318063\n",
      "=== epoch:147, train acc:0.486666666667, test acc:0.4004 ===\n",
      "train loss:2.07360559626\n",
      "train loss:2.0697598334\n",
      "train loss:2.06324378487\n",
      "=== epoch:148, train acc:0.493333333333, test acc:0.4055 ===\n",
      "train loss:2.03619721557\n",
      "train loss:2.03332196348\n",
      "train loss:1.98300602727\n",
      "=== epoch:149, train acc:0.496666666667, test acc:0.4067 ===\n",
      "train loss:2.03852661997\n",
      "train loss:2.00600299342\n",
      "train loss:2.0064385521\n",
      "=== epoch:150, train acc:0.496666666667, test acc:0.4131 ===\n",
      "train loss:1.94082236532\n",
      "train loss:2.03434690259\n",
      "train loss:2.01078981051\n",
      "=== epoch:151, train acc:0.493333333333, test acc:0.4166 ===\n",
      "train loss:1.9971227665\n",
      "train loss:2.00359277209\n",
      "train loss:1.97013174328\n",
      "=== epoch:152, train acc:0.5, test acc:0.4142 ===\n",
      "train loss:1.964390827\n",
      "train loss:1.99311349616\n",
      "train loss:1.98366971958\n",
      "=== epoch:153, train acc:0.5, test acc:0.4157 ===\n",
      "train loss:1.96395024879\n",
      "train loss:2.00625623165\n",
      "train loss:2.023053499\n",
      "=== epoch:154, train acc:0.506666666667, test acc:0.421 ===\n",
      "train loss:2.03686366205\n",
      "train loss:1.97353261942\n",
      "train loss:2.02284817049\n",
      "=== epoch:155, train acc:0.503333333333, test acc:0.4234 ===\n",
      "train loss:1.98208731273\n",
      "train loss:1.98121641684\n",
      "train loss:1.93607538265\n",
      "=== epoch:156, train acc:0.496666666667, test acc:0.4211 ===\n",
      "train loss:1.98105365196\n",
      "train loss:2.01221129991\n",
      "train loss:1.93060920862\n",
      "=== epoch:157, train acc:0.49, test acc:0.4194 ===\n",
      "train loss:2.01095574749\n",
      "train loss:1.94850071843\n",
      "train loss:1.94383589314\n",
      "=== epoch:158, train acc:0.5, test acc:0.4242 ===\n",
      "train loss:1.91222061199\n",
      "train loss:1.92679758679\n",
      "train loss:1.98490372916\n",
      "=== epoch:159, train acc:0.496666666667, test acc:0.4217 ===\n",
      "train loss:1.97002351184\n",
      "train loss:1.94590002865\n",
      "train loss:2.05255815416\n",
      "=== epoch:160, train acc:0.493333333333, test acc:0.4233 ===\n",
      "train loss:1.93581940874\n",
      "train loss:2.01720441286\n",
      "train loss:1.98450537474\n",
      "=== epoch:161, train acc:0.493333333333, test acc:0.4247 ===\n",
      "train loss:1.92737143655\n",
      "train loss:1.94447863098\n",
      "train loss:1.89124974989\n",
      "=== epoch:162, train acc:0.496666666667, test acc:0.4235 ===\n",
      "train loss:1.94680983991\n",
      "train loss:1.93918488907\n",
      "train loss:1.93826083411\n",
      "=== epoch:163, train acc:0.496666666667, test acc:0.424 ===\n",
      "train loss:1.97138348429\n",
      "train loss:1.97259685103\n",
      "train loss:1.90684291284\n",
      "=== epoch:164, train acc:0.5, test acc:0.4258 ===\n",
      "train loss:1.8257060564\n",
      "train loss:1.9400942492\n",
      "train loss:1.98041127336\n",
      "=== epoch:165, train acc:0.496666666667, test acc:0.4294 ===\n",
      "train loss:1.93725990411\n",
      "train loss:1.90965647359\n",
      "train loss:1.93315398176\n",
      "=== epoch:166, train acc:0.496666666667, test acc:0.432 ===\n",
      "train loss:1.92337049907\n",
      "train loss:1.97813725954\n",
      "train loss:1.90649093074\n",
      "=== epoch:167, train acc:0.5, test acc:0.4346 ===\n",
      "train loss:1.88127636314\n",
      "train loss:1.93043391576\n",
      "train loss:1.83076674367\n",
      "=== epoch:168, train acc:0.503333333333, test acc:0.4323 ===\n",
      "train loss:1.91301207473\n",
      "train loss:1.97047803805\n",
      "train loss:1.97145595861\n",
      "=== epoch:169, train acc:0.513333333333, test acc:0.437 ===\n",
      "train loss:1.82579943283\n",
      "train loss:1.93924322035\n",
      "train loss:1.94375192906\n",
      "=== epoch:170, train acc:0.513333333333, test acc:0.4399 ===\n",
      "train loss:1.85744802407\n",
      "train loss:1.92821422662\n",
      "train loss:1.90952856367\n",
      "=== epoch:171, train acc:0.523333333333, test acc:0.4433 ===\n",
      "train loss:1.90478856034\n",
      "train loss:1.91453670011\n",
      "train loss:1.88386538771\n",
      "=== epoch:172, train acc:0.51, test acc:0.4379 ===\n",
      "train loss:1.89165528196\n",
      "train loss:1.83665501327\n",
      "train loss:1.820462786\n",
      "=== epoch:173, train acc:0.503333333333, test acc:0.437 ===\n",
      "train loss:1.84596085598\n",
      "train loss:1.93047809077\n",
      "train loss:1.8310938062\n",
      "=== epoch:174, train acc:0.51, test acc:0.437 ===\n",
      "train loss:1.88491557129\n",
      "train loss:1.87825822527\n",
      "train loss:1.90565115056\n",
      "=== epoch:175, train acc:0.51, test acc:0.4399 ===\n",
      "train loss:1.8804027339\n",
      "train loss:1.91188559275\n",
      "train loss:1.90955341462\n",
      "=== epoch:176, train acc:0.51, test acc:0.4386 ===\n",
      "train loss:1.84189761291\n",
      "train loss:1.87669596893\n",
      "train loss:1.88869618584\n",
      "=== epoch:177, train acc:0.51, test acc:0.4422 ===\n",
      "train loss:1.87330928108\n",
      "train loss:1.82624730353\n",
      "train loss:1.8613783056\n",
      "=== epoch:178, train acc:0.51, test acc:0.4413 ===\n",
      "train loss:1.87744128174\n",
      "train loss:1.89292646404\n",
      "train loss:1.85759605344\n",
      "=== epoch:179, train acc:0.52, test acc:0.4414 ===\n",
      "train loss:1.88807993688\n",
      "train loss:1.86713399516\n",
      "train loss:1.87011339404\n",
      "=== epoch:180, train acc:0.516666666667, test acc:0.4439 ===\n",
      "train loss:1.86738607417\n",
      "train loss:1.88727001616\n",
      "train loss:1.83609277769\n",
      "=== epoch:181, train acc:0.52, test acc:0.4479 ===\n",
      "train loss:1.85384073089\n",
      "train loss:1.79453361104\n",
      "train loss:1.74619155787\n",
      "=== epoch:182, train acc:0.52, test acc:0.4468 ===\n",
      "train loss:1.80000086258\n",
      "train loss:1.92885876304\n",
      "train loss:1.86095230158\n",
      "=== epoch:183, train acc:0.516666666667, test acc:0.4488 ===\n",
      "train loss:1.75678224166\n",
      "train loss:1.75756669431\n",
      "train loss:1.8808700588\n",
      "=== epoch:184, train acc:0.513333333333, test acc:0.4468 ===\n",
      "train loss:1.82516103418\n",
      "train loss:1.79385568907\n",
      "train loss:1.85248477573\n",
      "=== epoch:185, train acc:0.513333333333, test acc:0.4501 ===\n",
      "train loss:1.75900417637\n",
      "train loss:1.77706517227\n",
      "train loss:1.91088681112\n",
      "=== epoch:186, train acc:0.513333333333, test acc:0.4524 ===\n",
      "train loss:1.81595024265\n",
      "train loss:1.78871290216\n",
      "train loss:1.75206196545\n",
      "=== epoch:187, train acc:0.516666666667, test acc:0.4502 ===\n",
      "train loss:1.8745421341\n",
      "train loss:1.72188086087\n",
      "train loss:1.70832640434\n",
      "=== epoch:188, train acc:0.516666666667, test acc:0.4531 ===\n",
      "train loss:1.80074871365\n",
      "train loss:1.7449832592\n",
      "train loss:1.8402078184\n",
      "=== epoch:189, train acc:0.526666666667, test acc:0.4524 ===\n",
      "train loss:1.89047421955\n",
      "train loss:1.7338418039\n",
      "train loss:1.82100415529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:190, train acc:0.523333333333, test acc:0.4538 ===\n",
      "train loss:1.77349154414\n",
      "train loss:1.76722390134\n",
      "train loss:1.73102143372\n",
      "=== epoch:191, train acc:0.516666666667, test acc:0.4521 ===\n",
      "train loss:1.8510422707\n",
      "train loss:1.76586994162\n",
      "train loss:1.79823380109\n",
      "=== epoch:192, train acc:0.516666666667, test acc:0.4517 ===\n",
      "train loss:1.71989115194\n",
      "train loss:1.70806188675\n",
      "train loss:1.69709597211\n",
      "=== epoch:193, train acc:0.53, test acc:0.4527 ===\n",
      "train loss:1.7888980351\n",
      "train loss:1.70480366569\n",
      "train loss:1.75949786802\n",
      "=== epoch:194, train acc:0.53, test acc:0.4571 ===\n",
      "train loss:1.75194986839\n",
      "train loss:1.80553822885\n",
      "train loss:1.79998431563\n",
      "=== epoch:195, train acc:0.53, test acc:0.4581 ===\n",
      "train loss:1.69652704199\n",
      "train loss:1.77768123788\n",
      "train loss:1.70305848883\n",
      "=== epoch:196, train acc:0.53, test acc:0.4543 ===\n",
      "train loss:1.8164340984\n",
      "train loss:1.63803649185\n",
      "train loss:1.69709105019\n",
      "=== epoch:197, train acc:0.53, test acc:0.4606 ===\n",
      "train loss:1.69740364705\n",
      "train loss:1.68117615282\n",
      "train loss:1.75063088991\n",
      "=== epoch:198, train acc:0.53, test acc:0.4588 ===\n",
      "train loss:1.69091347902\n",
      "train loss:1.73072899786\n",
      "train loss:1.62809561868\n",
      "=== epoch:199, train acc:0.52, test acc:0.4595 ===\n",
      "train loss:1.73488925654\n",
      "train loss:1.71123820188\n",
      "train loss:1.75949494823\n",
      "=== epoch:200, train acc:0.523333333333, test acc:0.4598 ===\n",
      "train loss:1.71531553773\n",
      "train loss:1.79649098116\n",
      "train loss:1.69916310474\n",
      "=== epoch:201, train acc:0.526666666667, test acc:0.4601 ===\n",
      "train loss:1.62504555904\n",
      "train loss:1.775908127\n",
      "train loss:1.79742287912\n",
      "=== epoch:202, train acc:0.53, test acc:0.4591 ===\n",
      "train loss:1.66117275695\n",
      "train loss:1.79009673086\n",
      "train loss:1.71978865612\n",
      "=== epoch:203, train acc:0.536666666667, test acc:0.4547 ===\n",
      "train loss:1.69952618647\n",
      "train loss:1.66414635396\n",
      "train loss:1.73640717703\n",
      "=== epoch:204, train acc:0.526666666667, test acc:0.4547 ===\n",
      "train loss:1.72914628115\n",
      "train loss:1.64708615629\n",
      "train loss:1.74575594737\n",
      "=== epoch:205, train acc:0.54, test acc:0.4559 ===\n",
      "train loss:1.74073200973\n",
      "train loss:1.66845579338\n",
      "train loss:1.6671445494\n",
      "=== epoch:206, train acc:0.536666666667, test acc:0.4588 ===\n",
      "train loss:1.64321142515\n",
      "train loss:1.70986970002\n",
      "train loss:1.71562019803\n",
      "=== epoch:207, train acc:0.55, test acc:0.4562 ===\n",
      "train loss:1.74623235591\n",
      "train loss:1.64853646363\n",
      "train loss:1.77588807439\n",
      "=== epoch:208, train acc:0.536666666667, test acc:0.4569 ===\n",
      "train loss:1.66864487186\n",
      "train loss:1.69950997366\n",
      "train loss:1.68961325133\n",
      "=== epoch:209, train acc:0.54, test acc:0.4557 ===\n",
      "train loss:1.75028263645\n",
      "train loss:1.65322057073\n",
      "train loss:1.62541860836\n",
      "=== epoch:210, train acc:0.533333333333, test acc:0.4585 ===\n",
      "train loss:1.68520532763\n",
      "train loss:1.72372993661\n",
      "train loss:1.62166635137\n",
      "=== epoch:211, train acc:0.533333333333, test acc:0.4578 ===\n",
      "train loss:1.62130931501\n",
      "train loss:1.70380929004\n",
      "train loss:1.60502640176\n",
      "=== epoch:212, train acc:0.536666666667, test acc:0.4543 ===\n",
      "train loss:1.68978096966\n",
      "train loss:1.67242822122\n",
      "train loss:1.65664084063\n",
      "=== epoch:213, train acc:0.543333333333, test acc:0.4568 ===\n",
      "train loss:1.68658227662\n",
      "train loss:1.53396341398\n",
      "train loss:1.54497895674\n",
      "=== epoch:214, train acc:0.54, test acc:0.4555 ===\n",
      "train loss:1.60467190813\n",
      "train loss:1.70546542033\n",
      "train loss:1.55884176086\n",
      "=== epoch:215, train acc:0.54, test acc:0.4565 ===\n",
      "train loss:1.59503264131\n",
      "train loss:1.64999278606\n",
      "train loss:1.63573240555\n",
      "=== epoch:216, train acc:0.54, test acc:0.4513 ===\n",
      "train loss:1.64395081864\n",
      "train loss:1.68742878036\n",
      "train loss:1.60009235489\n",
      "=== epoch:217, train acc:0.546666666667, test acc:0.456 ===\n",
      "train loss:1.66290436728\n",
      "train loss:1.6195723647\n",
      "train loss:1.62334223463\n",
      "=== epoch:218, train acc:0.543333333333, test acc:0.459 ===\n",
      "train loss:1.65262538034\n",
      "train loss:1.49883523886\n",
      "train loss:1.59079308228\n",
      "=== epoch:219, train acc:0.546666666667, test acc:0.4546 ===\n",
      "train loss:1.65022159931\n",
      "train loss:1.62357042244\n",
      "train loss:1.58963557431\n",
      "=== epoch:220, train acc:0.543333333333, test acc:0.456 ===\n",
      "train loss:1.61464917569\n",
      "train loss:1.542570091\n",
      "train loss:1.58724923969\n",
      "=== epoch:221, train acc:0.546666666667, test acc:0.4561 ===\n",
      "train loss:1.53133902778\n",
      "train loss:1.58484026169\n",
      "train loss:1.57542459922\n",
      "=== epoch:222, train acc:0.553333333333, test acc:0.4583 ===\n",
      "train loss:1.60541900667\n",
      "train loss:1.64740209602\n",
      "train loss:1.52361673605\n",
      "=== epoch:223, train acc:0.55, test acc:0.4594 ===\n",
      "train loss:1.52800221912\n",
      "train loss:1.48917905612\n",
      "train loss:1.52358323878\n",
      "=== epoch:224, train acc:0.543333333333, test acc:0.4556 ===\n",
      "train loss:1.58217248357\n",
      "train loss:1.56009277305\n",
      "train loss:1.52791293036\n",
      "=== epoch:225, train acc:0.56, test acc:0.4607 ===\n",
      "train loss:1.59658377783\n",
      "train loss:1.56113342191\n",
      "train loss:1.56299058377\n",
      "=== epoch:226, train acc:0.563333333333, test acc:0.465 ===\n",
      "train loss:1.46352108553\n",
      "train loss:1.74895675491\n",
      "train loss:1.51502780812\n",
      "=== epoch:227, train acc:0.553333333333, test acc:0.4633 ===\n",
      "train loss:1.52422692036\n",
      "train loss:1.53900617965\n",
      "train loss:1.60240463394\n",
      "=== epoch:228, train acc:0.56, test acc:0.4668 ===\n",
      "train loss:1.47178905147\n",
      "train loss:1.64600334728\n",
      "train loss:1.43576210753\n",
      "=== epoch:229, train acc:0.56, test acc:0.4622 ===\n",
      "train loss:1.42717688758\n",
      "train loss:1.51885492772\n",
      "train loss:1.47681930265\n",
      "=== epoch:230, train acc:0.543333333333, test acc:0.4583 ===\n",
      "train loss:1.55095487246\n",
      "train loss:1.4817889585\n",
      "train loss:1.53687751617\n",
      "=== epoch:231, train acc:0.543333333333, test acc:0.4564 ===\n",
      "train loss:1.57672147505\n",
      "train loss:1.54391017031\n",
      "train loss:1.5855990207\n",
      "=== epoch:232, train acc:0.55, test acc:0.4598 ===\n",
      "train loss:1.45517098019\n",
      "train loss:1.56438572099\n",
      "train loss:1.47956091122\n",
      "=== epoch:233, train acc:0.54, test acc:0.4587 ===\n",
      "train loss:1.48216419874\n",
      "train loss:1.3484231981\n",
      "train loss:1.45835558526\n",
      "=== epoch:234, train acc:0.566666666667, test acc:0.4631 ===\n",
      "train loss:1.4929284853\n",
      "train loss:1.55024920777\n",
      "train loss:1.4154630331\n",
      "=== epoch:235, train acc:0.563333333333, test acc:0.4604 ===\n",
      "train loss:1.52280042171\n",
      "train loss:1.56679593431\n",
      "train loss:1.56047804578\n",
      "=== epoch:236, train acc:0.566666666667, test acc:0.4618 ===\n",
      "train loss:1.60882705256\n",
      "train loss:1.51456400601\n",
      "train loss:1.45847077145\n",
      "=== epoch:237, train acc:0.556666666667, test acc:0.4603 ===\n",
      "train loss:1.47132157605\n",
      "train loss:1.52721753277\n",
      "train loss:1.51440721943\n",
      "=== epoch:238, train acc:0.553333333333, test acc:0.4583 ===\n",
      "train loss:1.50950864885\n",
      "train loss:1.45436097447\n",
      "train loss:1.53799465113\n",
      "=== epoch:239, train acc:0.53, test acc:0.4549 ===\n",
      "train loss:1.44595403043\n",
      "train loss:1.36627238016\n",
      "train loss:1.49169184143\n",
      "=== epoch:240, train acc:0.536666666667, test acc:0.4528 ===\n",
      "train loss:1.35850001097\n",
      "train loss:1.56433344757\n",
      "train loss:1.47667607193\n",
      "=== epoch:241, train acc:0.536666666667, test acc:0.4519 ===\n",
      "train loss:1.46977861937\n",
      "train loss:1.54346090285\n",
      "train loss:1.36259806463\n",
      "=== epoch:242, train acc:0.543333333333, test acc:0.4499 ===\n",
      "train loss:1.47489336024\n",
      "train loss:1.41848784114\n",
      "train loss:1.53496801911\n",
      "=== epoch:243, train acc:0.54, test acc:0.4478 ===\n",
      "train loss:1.39553006061\n",
      "train loss:1.42955960014\n",
      "train loss:1.48301055304\n",
      "=== epoch:244, train acc:0.546666666667, test acc:0.4551 ===\n",
      "train loss:1.61791974336\n",
      "train loss:1.4091533849\n",
      "train loss:1.44531459875\n",
      "=== epoch:245, train acc:0.543333333333, test acc:0.4543 ===\n",
      "train loss:1.52152809493\n",
      "train loss:1.36976789902\n",
      "train loss:1.43596089362\n",
      "=== epoch:246, train acc:0.54, test acc:0.4559 ===\n",
      "train loss:1.41912113417\n",
      "train loss:1.42204969344\n",
      "train loss:1.45647000729\n",
      "=== epoch:247, train acc:0.546666666667, test acc:0.4582 ===\n",
      "train loss:1.43476845737\n",
      "train loss:1.44177769361\n",
      "train loss:1.47418133333\n",
      "=== epoch:248, train acc:0.55, test acc:0.4593 ===\n",
      "train loss:1.28803310442\n",
      "train loss:1.34547150533\n",
      "train loss:1.28382424814\n",
      "=== epoch:249, train acc:0.553333333333, test acc:0.461 ===\n",
      "train loss:1.24655587345\n",
      "train loss:1.35038685763\n",
      "train loss:1.36179854371\n",
      "=== epoch:250, train acc:0.543333333333, test acc:0.4569 ===\n",
      "train loss:1.30181608579\n",
      "train loss:1.3309627271\n",
      "train loss:1.30678532383\n",
      "=== epoch:251, train acc:0.546666666667, test acc:0.4559 ===\n",
      "train loss:1.41766227264\n",
      "train loss:1.46277637187\n",
      "train loss:1.32746315224\n",
      "=== epoch:252, train acc:0.553333333333, test acc:0.4601 ===\n",
      "train loss:1.39434721249\n",
      "train loss:1.25551623745\n",
      "train loss:1.37995570302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:253, train acc:0.546666666667, test acc:0.4619 ===\n",
      "train loss:1.46366080812\n",
      "train loss:1.24790666222\n",
      "train loss:1.39887858169\n",
      "=== epoch:254, train acc:0.553333333333, test acc:0.4579 ===\n",
      "train loss:1.44240377895\n",
      "train loss:1.38034310804\n",
      "train loss:1.27606168334\n",
      "=== epoch:255, train acc:0.546666666667, test acc:0.458 ===\n",
      "train loss:1.49653145405\n",
      "train loss:1.38373087117\n",
      "train loss:1.34653797359\n",
      "=== epoch:256, train acc:0.546666666667, test acc:0.4577 ===\n",
      "train loss:1.38999827277\n",
      "train loss:1.3346425654\n",
      "train loss:1.3560966794\n",
      "=== epoch:257, train acc:0.553333333333, test acc:0.4649 ===\n",
      "train loss:1.36274116959\n",
      "train loss:1.37160659146\n",
      "train loss:1.26503427908\n",
      "=== epoch:258, train acc:0.553333333333, test acc:0.4629 ===\n",
      "train loss:1.25809627341\n",
      "train loss:1.34158733654\n",
      "train loss:1.31769334594\n",
      "=== epoch:259, train acc:0.56, test acc:0.4579 ===\n",
      "train loss:1.18869625512\n",
      "train loss:1.45671961231\n",
      "train loss:1.37925953767\n",
      "=== epoch:260, train acc:0.56, test acc:0.4648 ===\n",
      "train loss:1.35374302123\n",
      "train loss:1.35912395126\n",
      "train loss:1.32013500775\n",
      "=== epoch:261, train acc:0.56, test acc:0.4655 ===\n",
      "train loss:1.36427184878\n",
      "train loss:1.22612609787\n",
      "train loss:1.15886069925\n",
      "=== epoch:262, train acc:0.576666666667, test acc:0.4707 ===\n",
      "train loss:1.3163869312\n",
      "train loss:1.35658590663\n",
      "train loss:1.23787326712\n",
      "=== epoch:263, train acc:0.57, test acc:0.4698 ===\n",
      "train loss:1.25254632649\n",
      "train loss:1.33494308214\n",
      "train loss:1.41582533904\n",
      "=== epoch:264, train acc:0.576666666667, test acc:0.4715 ===\n",
      "train loss:1.30249169674\n",
      "train loss:1.37601367742\n",
      "train loss:1.23828532294\n",
      "=== epoch:265, train acc:0.586666666667, test acc:0.474 ===\n",
      "train loss:1.45437702063\n",
      "train loss:1.30960067885\n",
      "train loss:1.31454491606\n",
      "=== epoch:266, train acc:0.573333333333, test acc:0.4748 ===\n",
      "train loss:1.26939842686\n",
      "train loss:1.47095705733\n",
      "train loss:1.44827287409\n",
      "=== epoch:267, train acc:0.583333333333, test acc:0.4761 ===\n",
      "train loss:1.2303026952\n",
      "train loss:1.2947510874\n",
      "train loss:1.32766124909\n",
      "=== epoch:268, train acc:0.59, test acc:0.4794 ===\n",
      "train loss:1.29696252056\n",
      "train loss:1.31212106026\n",
      "train loss:1.33290933544\n",
      "=== epoch:269, train acc:0.606666666667, test acc:0.4856 ===\n",
      "train loss:1.30239162192\n",
      "train loss:1.36009597188\n",
      "train loss:1.23044152882\n",
      "=== epoch:270, train acc:0.596666666667, test acc:0.4814 ===\n",
      "train loss:1.36497710337\n",
      "train loss:1.3396110702\n",
      "train loss:1.42363046786\n",
      "=== epoch:271, train acc:0.596666666667, test acc:0.4873 ===\n",
      "train loss:1.21671534662\n",
      "train loss:1.1762200539\n",
      "train loss:1.29265334394\n",
      "=== epoch:272, train acc:0.593333333333, test acc:0.4899 ===\n",
      "train loss:1.19117375899\n",
      "train loss:1.2196080775\n",
      "train loss:1.32964817876\n",
      "=== epoch:273, train acc:0.596666666667, test acc:0.489 ===\n",
      "train loss:1.17618192928\n",
      "train loss:1.18010404546\n",
      "train loss:1.18882379338\n",
      "=== epoch:274, train acc:0.603333333333, test acc:0.4907 ===\n",
      "train loss:1.29497006907\n",
      "train loss:1.30163495161\n",
      "train loss:1.0989729055\n",
      "=== epoch:275, train acc:0.596666666667, test acc:0.4892 ===\n",
      "train loss:1.1724118672\n",
      "train loss:1.1968067337\n",
      "train loss:1.30156500403\n",
      "=== epoch:276, train acc:0.61, test acc:0.4886 ===\n",
      "train loss:1.25710032906\n",
      "train loss:1.09829117026\n",
      "train loss:1.28938998141\n",
      "=== epoch:277, train acc:0.613333333333, test acc:0.489 ===\n",
      "train loss:1.2738303127\n",
      "train loss:1.16375075749\n",
      "train loss:1.24818684123\n",
      "=== epoch:278, train acc:0.613333333333, test acc:0.492 ===\n",
      "train loss:1.19993334093\n",
      "train loss:1.1701613148\n",
      "train loss:1.16162447522\n",
      "=== epoch:279, train acc:0.606666666667, test acc:0.4906 ===\n",
      "train loss:1.23394026058\n",
      "train loss:1.14036253546\n",
      "train loss:1.16677122118\n",
      "=== epoch:280, train acc:0.6, test acc:0.4876 ===\n",
      "train loss:1.22380812808\n",
      "train loss:1.06313341833\n",
      "train loss:1.25271647382\n",
      "=== epoch:281, train acc:0.606666666667, test acc:0.4905 ===\n",
      "train loss:1.07698866612\n",
      "train loss:1.13763966333\n",
      "train loss:1.25288786363\n",
      "=== epoch:282, train acc:0.606666666667, test acc:0.4902 ===\n",
      "train loss:1.1010793075\n",
      "train loss:1.19752101834\n",
      "train loss:1.29644143388\n",
      "=== epoch:283, train acc:0.61, test acc:0.4929 ===\n",
      "train loss:1.22647085977\n",
      "train loss:1.03656703365\n",
      "train loss:1.20595045393\n",
      "=== epoch:284, train acc:0.61, test acc:0.496 ===\n",
      "train loss:1.07519230006\n",
      "train loss:1.19489868575\n",
      "train loss:1.21153222023\n",
      "=== epoch:285, train acc:0.62, test acc:0.4998 ===\n",
      "train loss:1.21788604923\n",
      "train loss:1.23219666605\n",
      "train loss:1.23499434521\n",
      "=== epoch:286, train acc:0.616666666667, test acc:0.5065 ===\n",
      "train loss:1.2023891994\n",
      "train loss:1.22243334377\n",
      "train loss:1.21016026395\n",
      "=== epoch:287, train acc:0.62, test acc:0.5087 ===\n",
      "train loss:1.15635822435\n",
      "train loss:1.24124432874\n",
      "train loss:1.13621161496\n",
      "=== epoch:288, train acc:0.616666666667, test acc:0.5045 ===\n",
      "train loss:1.2286749846\n",
      "train loss:1.24837368259\n",
      "train loss:1.28803451879\n",
      "=== epoch:289, train acc:0.613333333333, test acc:0.5036 ===\n",
      "train loss:1.19297463053\n",
      "train loss:1.08994783294\n",
      "train loss:1.13356950439\n",
      "=== epoch:290, train acc:0.616666666667, test acc:0.5054 ===\n",
      "train loss:1.290488545\n",
      "train loss:0.982990027249\n",
      "train loss:1.24409415672\n",
      "=== epoch:291, train acc:0.623333333333, test acc:0.5084 ===\n",
      "train loss:1.15753041081\n",
      "train loss:1.0404655347\n",
      "train loss:1.12968593719\n",
      "=== epoch:292, train acc:0.616666666667, test acc:0.5041 ===\n",
      "train loss:1.07842211568\n",
      "train loss:1.08162983943\n",
      "train loss:1.26403904473\n",
      "=== epoch:293, train acc:0.613333333333, test acc:0.5067 ===\n",
      "train loss:1.10627624637\n",
      "train loss:1.20318418385\n",
      "train loss:1.03135547486\n",
      "=== epoch:294, train acc:0.616666666667, test acc:0.5098 ===\n",
      "train loss:1.13080525934\n",
      "train loss:1.15680623121\n",
      "train loss:1.28934154892\n",
      "=== epoch:295, train acc:0.616666666667, test acc:0.5118 ===\n",
      "train loss:0.967481370673\n",
      "train loss:1.12579080995\n",
      "train loss:1.11840899388\n",
      "=== epoch:296, train acc:0.606666666667, test acc:0.5058 ===\n",
      "train loss:1.08085521357\n",
      "train loss:0.982346001095\n",
      "train loss:1.10327249423\n",
      "=== epoch:297, train acc:0.61, test acc:0.5091 ===\n",
      "train loss:1.12328483115\n",
      "train loss:1.11710705793\n",
      "train loss:1.31009742103\n",
      "=== epoch:298, train acc:0.613333333333, test acc:0.5104 ===\n",
      "train loss:1.17853394909\n",
      "train loss:1.22562343328\n",
      "train loss:1.12121101574\n",
      "=== epoch:299, train acc:0.61, test acc:0.5123 ===\n",
      "train loss:1.12159742854\n",
      "train loss:1.02899378334\n",
      "train loss:1.17350730783\n",
      "=== epoch:300, train acc:0.616666666667, test acc:0.5159 ===\n",
      "train loss:1.09848251508\n",
      "train loss:1.07796647905\n",
      "train loss:1.14977541546\n",
      "=== epoch:301, train acc:0.633333333333, test acc:0.5177 ===\n",
      "train loss:1.07936007239\n",
      "train loss:1.18747730206\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5176\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wfxtK16EY4l2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d747550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)  # \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# \n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# Dropuout ========================\n",
    "use_dropout = True  # DropoutFalse\n",
    "dropout_ratio = 0.2\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
    "\n",
    "# ==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
